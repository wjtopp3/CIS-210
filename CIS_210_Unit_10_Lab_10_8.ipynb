{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPP/J8sCBzHvn+rA5i1NGjd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wjtopp3/CIS-210/blob/main/CIS_210_Unit_10_Lab_10_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *diamonds.csv* dataset contains the price, cut, color, and other characteristics of a sample of diamonds. The dataframe X contains *all the features except cut, color, clarity, and price*. The dataframe y contains the feature *price*.\n",
        "\n",
        "- Define a standardization scaler and apply the scaler to Xtrain and Xtest.\n",
        "- Initialize and fit a *multilayer perceptron regressor* with with ***random_state=42, three hidden layers of 50 nodes each, an adaptive learning rate of 0.01, a batch size of 100, and a maximum of 300 iterations***.\n",
        "- Print the ***R-squared scores*** for the training data and the testing data, rounded to the fourth decimal place.\n",
        "\n",
        "Ex: If random_state=123 is used instead of random_state=42, the output is:\n",
        "\n",
        "Score for the training data:  0.8627\n",
        "\n",
        "Score for the testing data:  0.8761"
      ],
      "metadata": {
        "id": "Rro81IT6GeVt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT5k1he1F4HB"
      },
      "outputs": [],
      "source": [
        "# zyBooks template\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "diamonds = pd.read_csv('diamonds.csv')\n",
        "diamond_sample = diamonds.sample(1000, random_state=123)\n",
        "\n",
        "X = diamond_sample.drop(columns=['cut', 'color', 'clarity', 'price'])\n",
        "y = diamond_sample[['price']]\n",
        "\n",
        "# Split data into train and test sets\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=123)\n",
        "\n",
        "# Define a standardization scaler to transform values\n",
        "# Your code here\n",
        "\n",
        "# Apply scaler\n",
        "Xtrain = # Your code here\n",
        "Xtest = # Your code here\n",
        "\n",
        "# Initialize a multilayer perceptron regressor with random_state=42, three hidden layers of 50 nodes each,\n",
        "# an adaptive learning rate of 0.01, a batch size of 100, and a maximum of 300 iterations\n",
        "mlpDiamond = # Your code here\n",
        "\n",
        "# Fit the model to the training data\n",
        "# Your code here\n",
        "\n",
        "# Print the R-squared score for the training data\n",
        "print(\"Score for the training data: \", round(# Your code here, 4))\n",
        "# Print the R-squared score for the testing data\n",
        "print(\"Score for the testing data: \", round(# Your code here, 4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# zyBooks solution\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "diamonds = pd.read_csv('diamonds.csv')\n",
        "diamond_sample = diamonds.sample(1000, random_state=123)\n",
        "\n",
        "X = diamond_sample.drop(columns=['cut', 'color', 'clarity', 'price'])\n",
        "y = diamond_sample[['price']]\n",
        "\n",
        "# Split data into train and test sets\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=123)\n",
        "\n",
        "# Define a standardization scaler to transform values\n",
        "standard_scaler = StandardScaler()\n",
        "\n",
        "# Apply scaler\n",
        "Xtrain = standard_scaler.fit_transform(Xtrain)\n",
        "Xtest = standard_scaler.transform(Xtest)\n",
        "\n",
        "# Initialize a multilayer perceptron regressor with random_state=42, three hidden layers of 50 nodes each,\n",
        "# an adaptive learning rate of 0.01, a batch size of 100, and a maximum of 300 iterations\n",
        "mlpDiamond = MLPRegressor(random_state=42, hidden_layer_sizes=[50, 50, 50],  learning_rate_init=0.01, learning_rate = 'adaptive', batch_size = 100, max_iter=300)\n",
        "# Fit the model to the training data\n",
        "mlpDiamond.fit(Xtrain, np.ravel(ytrain))\n",
        "\n",
        "# Print the R-squared score for the training data\n",
        "print(\"Score for the training data: \", round(mlpDiamond.score(Xtrain, ytrain), 4))\n",
        "# Print the R-squared score for the testing data\n",
        "print(\"Score for the testing data: \", round(mlpDiamond.score(Xtest, ytest), 4))"
      ],
      "metadata": {
        "id": "cfUTdcn2HIqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Claude solution\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "diamonds = pd.read_csv('diamonds.csv')\n",
        "\n",
        "# Create feature dataframe X (all features except cut, color, clarity, and price)\n",
        "X = diamonds.drop(['cut', 'color', 'clarity', 'price'], axis=1)\n",
        "\n",
        "# Create target dataframe y containing price\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
        "    X, y, random_state=42\n",
        ")\n",
        "\n",
        "# Define standardization scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on training data and transform both training and testing data\n",
        "Xtrain_scaled = scaler.fit_transform(Xtrain)\n",
        "Xtest_scaled = scaler.transform(Xtest)\n",
        "\n",
        "# Initialize and fit multilayer perceptron regressor\n",
        "mlp_model = MLPRegressor(\n",
        "    random_state=42,\n",
        "    hidden_layer_sizes=(50, 50, 50),\n",
        "    learning_rate='adaptive',\n",
        "    learning_rate_init=0.01,\n",
        "    batch_size=100,\n",
        "    max_iter=300\n",
        ")\n",
        "\n",
        "mlp_model.fit(Xtrain_scaled, ytrain)\n",
        "\n",
        "# Calculate and print R-squared scores\n",
        "train_score = mlp_model.score(Xtrain_scaled, ytrain)\n",
        "test_score = mlp_model.score(Xtest_scaled, ytest)\n",
        "\n",
        "print(f\"Score for the training data:  {round(train_score, 4)}\")\n",
        "print(f\"Score for the testing data:  {round(test_score, 4)}\")"
      ],
      "metadata": {
        "id": "NZ1TLw8cHK1_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}